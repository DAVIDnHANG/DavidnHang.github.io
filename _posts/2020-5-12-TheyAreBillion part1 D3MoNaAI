**Deep Neural Netowrk**
The environment is our game
agent is the click. Deep neural network that clicks stuff.
Every time the game does stuff it gets a reward.
Should I maximum the reward for actions given every possible state?
states are observation
   for step t
   observation o
   
   a) each steps tens of thousdand of time-steps and thousands of action speed
 action of my agent - select what action type
      a)who to issue that action to
      b)subset  of the agent unit
      c)where to target
      d)among location on the map or units whithin the camera view
      e)when to observe and act next
      f)NO FUCTION HUMAN CONSTRINT
      APM over 1000
      
      c) kullback-Leibler divergence between its output and human actions sampled from a collection of replays
      
      three different agent each initalized by supervised learning were subsequently trained with reinforcement learning
      
      a) imitation learning
         1)agent trained by supervised learning game
         2) trained to predict each action a_t conditioned either solely on s_t or also on z
      
      b) reinforcement learning
         1) train on reinforcement learning to max win rate
         2) gradient algorithm similar to advantage actor critic
         3) off-policy learning update current policy from experience generated by a previous policy
           a) the current and previous policies are highly unlikely to match over many steps
         4) temportal difference learning (TD(lambda))
         5)clipped importance sampling (V-trace)
         6) self-imitation algorithm UPGO
         7) to reduce variance during training only, the value 
         function is estimated using informatin from both the player and the opponent prespectives
      c) multi-agent learning
         1) each agent is initialized to the parameters of the supervised learning agent.
           a) agents also receive a penalty whenever their action probabilities differ from the supervised policy. 
           b) this human exploration ensures that a wide variety of relevant modes of play continue to be explored throughout training.
           
      d) league training 
            a) an algorithm for multi agent reinforccement learning
            b) self-play algorithms learn rapidly but chase cycle A-> b B-> c but a->c 
            c) fictitious self-play avoids cycles by computing a best response against a unifrom mixture of all previous policies
            d) the mixture converge
         1) main agent - utilize a prioritized fictitious self-play mechanism that adapts the mixture probabilities proportionally 
           to the win rate of each opponenet against the agent.
         2) fixed probability a main agent is selected as an opponent.
         3) second, main exploiter agent play only against the current iteration of main agents.
           a) to exploited weakness against current iteration of main agents
           b) so main agent has to deal with this
         4)league exploiter agents use a similar PFSP mechanism to the main agents, but are not targeted by main exploiter agents.
           a) find weakness of the whole system
         5)
      long short term memory system
      
      recurrent pointer network
      
      
