---
layout: post
title: Agents AI for They are Billion
---

**Deep Neural Network**

There exists steps of time-steps and thousands of action steps the agent needs to take. The agent needs to have the following properties:
 1. Who to issue that action to
 2. Subset of the agent unit
 3. Where to target
 4. Among location on the map or units within the camera view
 5. When to observe and act next
 6. No Function Human constraint
 7. APM over 1000
    
kullback-Leibler divergence between its output and human actions sampled from a collection of replays
Three different agent each initialized by supervised learning were subsequently trained with reinforcement learning.Imitation learning has the following properties.
 1. The agent is trained by supervised learning game.
 2. The agent is trained to predict each action a_t conditioned either solely on s_t or also on z.
    
Reinforcement learning should be the following:
 1. Train on reinforcement learning to max win rate
 2. Gradient algorithm similar to advantage actor critic
 3. Off-policy learning update current policy from experience generated by a previous policy
    * The current and previous policies are highly unlikely to match over many steps
 4. Temporal difference learning (TD(lambda))
    * Clipped importance sampling (V-trace)
    * Self-imitation algorithm UPGO
    * To reduce variance during training only, the value function is estimated using information from both the player and the opponent perspectives
 5. Multi-agent learning
    * Each agent is initialized to the parameters of the supervised learning agent.
    * Agents also receive a penalty whenever their action probabilities differ from the supervised policy.
 6. This human exploration ensures that a wide variety of relevant modes of play continue to be explored throughout training.
    * League training
    * An algorithm for multi agent reinforcement learning
    * Self-play algorithms learn rapidly but chase cycle A-> b B-> c but a->c
    * Fictitious self-play avoids cycles by computing a best response against a uniform mixture of all previous policies
    * The mixture converge
 7. Main agent - utilize a prioritized fictitious self-play mechanism that adapts the mixture probabilities proportionally to the win rate of each opponent against the agent.
 8. Fixed probability a main agent is selected as an opponent.
 9. Second, main exploiter agent play only against the current iteration of main agents.
 10. To exploited weakness against current iteration of main agents
     * So main agent has to deal with this
 11. League exploiter agents use a similar PFSP mechanism to the main agents, but are not targeted by main exploiter agents.
     * Find weakness of the whole system
     * Long short term memory system
     * Recurrent pointer network
    
Conclusion:
 I believe these are the agents needed to play a real time strategy game.
