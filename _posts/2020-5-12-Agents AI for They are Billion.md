---
layout: post
title: Agents AI for They are Billion
---

**Deep Neural Netowrk**

a. There exists steps of time-steps and thousands of action steps the  agent need to take. The agent needs to have the following properties:
  1. who to issue that action to
  2. subset of the agent unit
  3. where to target
  4. among location on the map or units within the camera view
  5. when to observe and act next
  6. No Function Human constraint
  7. APM over 1000
      
a. kullback-Leibler divergence between its output and human actions sampled from a collection of replays
      
b. three different agent each initialized by supervised learning were subsequently trained with reinforcement learning.
      
c. imitation learning
  1. agent trained by supervised learning game
  2. trained to predict each action a_t conditioned either solely on s_t or also on z
      
d. reinforcement learning
  1. train on reinforcement learning to max win rate
  2. gradient algorithm similar to advantage actor critic
  3. off-policy learning update current policy from experience generated by a previous policy
     * the current and previous policies are highly unlikely to match over many steps
  4. temporal difference learning (TD(lambda))
     * clipped importance sampling (V-trace)
     * self-imitation algorithm UPGO
     * to reduce variance during training only, the value function is estimated using information from both the player and the opponent perspectives
  5. multi-agent learning
     * each agent is initialized to the parameters of the supervised learning agent.
     * agents also receive a penalty whenever their action probabilities differ from the supervised policy. 
  6. this human exploration ensures that a wide variety of relevant modes of play continue to be explored throughout training.
     * league training 
     * an algorithm for multi agent reinforcement learning
     * self-play algorithms learn rapidly but chase cycle A-> b B-> c but a->c 
     * fictitious self-play avoids cycles by computing a best response against a uniform mixture of all previous policies
     * the mixture converge
  7. main agent - utilize a prioritized fictitious self-play mechanism that adapts the mixture probabilities proportionally to the win rate of each opponent against the agent.
  8. fixed probability a main agent is selected as an opponent.
  9. second, main exploiter agent play only against the current iteration of main agents.
  10. to exploited weakness against current iteration of main agents
      * so main agent has to deal with this
  11. league exploiter agents use a similar PFSP mechanism to the main agents, but are not targeted by main exploiter agents.
      * find weakness of the whole system
      * long short term memory system
      * recurrent pointer network
      
Conclusion:
  I believe these are the agents needed to play a real time strategy game.